---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: system-monitoring
spec:
  interval: 6h
  maxHistory: 2
  timeout: 20m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 44.2.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system

  values:
    ###
    ### Component values
    ###
    kubeApiServer:
      enabled: true

    kubeControllerManager:
      enabled: false

    kubeEtcd:
      enabled: true
      endpoints:
        - 192.168.80.91
        - 192.168.80.92
        - 192.168.80.93
      service:
        enabled: true
        port: 2381
        targetPort: 2381

    kubelet:
      enabled: true
      serviceMonitor:
        metricRelabelings:
          - action: replace
            sourceLabels:
              - node
            targetLabel: instance

    kubeProxy:
      enabled: false

    kubeScheduler:
      enabled: false

    kubeStateMetrics:
      enabled: true
    kube-state-metrics:
      metricLabelsAllowlist:
        - "persistentvolumeclaims=[*]"
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: kubernetes_node
      resources:
        requests:
          cpu: 15m
          memory: 127M
        limits:
          memory: 153M

    grafana:
      enabled: false
      forceDeployDashboards: true
      sidecar:
        dashboards:
          multicluster:
            etcd:
              enabled: true

    nodeExporter:
      enabled: true
    prometheus-node-exporter:
      resources:
        requests:
          cpu: 23m
          memory: 64M
        limits:
          memory: 64M

      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: kubernetes_node

    ###
    ### Prometheus operator values
    ###
    prometheusOperator:
      resources:
        requests:
          cpu: 35m
          memory: 273M
        limits:
          memory: 326M

      prometheusConfigReloader:
        # resource config for prometheusConfigReloader
        resources:
          requests:
            cpu: 11m
            memory: 32M
          limits:
            memory: 32M

    ###
    ### Prometheus instance values
    ###
    prometheus:
      ingress:
        enabled: true
        ingressClassName: nginx-internal
        pathType: Prefix
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt-production
        hosts:
          - prometheus.${CLUSTER_NAME}.${SECRET_DOMAIN}
        tls:
          - secretName: prometheus-${CLUSTER_NAME}-tls
            hosts:
              - prometheus.${CLUSTER_NAME}.${SECRET_DOMAIN}

      thanosService:
        enabled: true

      prometheusSpec:
        replicas: 1
        replicaExternalLabelName: "replica"
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        retentionSize: "6GB"
        retention: 2d
        enableAdminAPI: true
        walCompression: true
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-block
              resources:
                requests:
                  storage: 10Gi
        resources:
          requests:
            cpu: 250m
            memory: 4993M
          limits:
            memory: 5977M

        thanos:
          image: quay.io/thanos/thanos:v0.30.1
          # renovate: datasource=docker depName=quay.io/thanos/thanos
          version: v0.30.1
          objectStorageConfig:
            name: thanos-objstore-secret
            key: objstore.yml

        additionalScrapeConfigs:
          - job_name: node-exporter
            scrape_interval: 1m
            scrape_timeout: 10s
            honor_timestamps: true
            static_configs:
              - targets:
                  - "physical2.${SECRET_INTERNAL_DOMAIN}:9100"
                  - "talos-worker-1.${SECRET_INTERNAL_DOMAIN}:9100"
                  - "talos-worker-2.${SECRET_INTERNAL_DOMAIN}:9100"
                  - "physical1.${SECRET_INTERNAL_DOMAIN}:9100"
                  - "sff1.${SECRET_INTERNAL_DOMAIN}:9100"
          - job_name: "home-assistant"
            scrape_interval: 60s
            metrics_path: "/api/prometheus"
            bearer_token: "${SECRET_HASS_PROMETHEUS_TOKEN}"
            scheme: http
            static_configs:
              - targets:
                  - home-assistant.home.svc:8123
          - job_name: "opnsense"
            scrape_interval: 30s
            # metrics_path: "/metrics"
            # honor_timestamps: true
            scrape_timeout: 20s
            scheme: http
            static_configs:
              - targets:
                  - "opnsense.${SECRET_INTERNAL_DOMAIN}:9100"

          - job_name: "snmp"
            metrics_path: /snmp
            params:
              module: [if_mib]
            static_configs:
              - targets:
                  - "192.168.1.252"
            relabel_configs:
              - source_labels: [__address__]
                target_label: __param_target
              - source_labels: [__param_target]
                target_label: instance
              - target_label: __address__
                replacement: snmp-exporter-cisco-switch:9116 # SNMP exporter.

          - job_name: "windows_exporter"
            metrics_path: /metrics
            scrape_interval: 1m
            scrape_timeout: 10s
            honor_timestamps: true
            static_configs:
              - targets:
                  - "deskmonster.${SECRET_INTERNAL_DOMAIN}:9182"

    ###
    ### Alertmanager values
    ###
    # alertmanager:
    #   config:
    #     receivers:
    #       - name: "null"
    #       - name: "discord"
    #         slack_configs:
    #           - channel: "#prometheus"
    #             icon_url: https://avatars3.githubusercontent.com/u/3380462
    #             username: "Prometheus - ${CLUSTER_NAME}"
    #             send_resolved: true
    #             title: |-
    #               [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }} {{ else if ne .CommonAnnotations.message ""}}{{ .CommonAnnotations.message }} {{ else if ne .CommonAnnotations.description ""}}{{ .CommonAnnotations.description }} {{ else }}{{ .CommonLabels.alertname }}{{ end }}
    #             text: >-
    #               {{ range .Alerts -}}
    #                 *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
    #               {{ if ne .Annotations.summary ""}}*Summary:* {{ .Annotations.summary }} {{ else if ne .Annotations.message ""}}*Message:* {{ .Annotations.message }} {{ else if ne .Annotations.description ""}}*Description:* {{ .Annotations.description }}{{ end }}
    #               *Details:*
    #                 {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
    #                 {{ end }}
    #               {{ end }}

    #     route:
    #       group_by: ["alertname", "job"]
    #       group_wait: 30s
    #       group_interval: 5m
    #       repeat_interval: 6h
    #       receiver: "discord"
    #       routes:
    #         - receiver: "null"
    #           matchers:
    #             - alertname =~ "InfoInhibitor|Watchdog"
    #         - receiver: "discord"
    #           match_re:
    #             # severity: critical|warning
    #             severity: critical
    #           continue: true

    #     inhibit_rules:
    #       - source_match:
    #           severity: "critical"
    #         target_match:
    #           severity: "warning"
    #         equal: ["alertname", "namespace"]

    #   ingress:
    #     enabled: true
    #     ingressClassName: nginx
    #     pathType: Prefix
    #     annotations:
    #       cert-manager.io/cluster-issuer: letsencrypt-production
    #       external-dns.alpha.kubernetes.io/target: ingress.${SECRET_DOMAIN}
    #     hosts:
    #       - alertmanager.${CLUSTER_NAME}.${SECRET_DOMAIN}
    #     tls:
    #       - secretName: alertmanager-${CLUSTER_NAME}-tls
    #         hosts:
    #           - alertmanager.${CLUSTER_NAME}.${SECRET_DOMAIN}

    #   alertmanagerSpec:
    #     storage:
    #       volumeClaimTemplate:
    #         spec:
    #           storageClassName: ceph-block
    #           resources:
    #             requests:
    #               storage: 1Gi

    #     resources:
    #       requests:
    #         cpu: 11m
    #         memory: 50M
    #       limits:
    #         memory: 99M

    #   prometheus:
    #     monitor:
    #       enabled: true
    #       relabelings:
    #         - action: replace
    #           regex: (.*)
    #           replacement: $1
    #           sourceLabels:
    #             - __meta_kubernetes_pod_node_name
    #           targetLabel: kubernetes_node

  # valuesFrom:
  #   - kind: Secret
  #     name: kube-prometheus-stack
  #     valuesKey: discord-webhook
  #     targetPath: alertmanager.config.global.slack_api_url
  #     optional: false
